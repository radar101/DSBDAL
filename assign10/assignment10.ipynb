{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7eb1538-ea90-4b7e-a9e4-c3a1b9e76cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Darshan\n",
      "[nltk_data]     Mahajan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27cc01a8-8a1a-4c9a-bd82-9db99acb2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_directory = os.getcwd()\n",
    "os.chdir(script_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "827fd1ef-a81a-444a-82c2-83a471523a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fdaa3a0-f598-4879-9ab7-b058a64bffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path): \n",
    "    with open(file_path, 'r') as f: \n",
    "        word = f.read()\n",
    "        temp = nltk.word_tokenize(word)\n",
    "        all_tokens.extend(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0990177d-1b32-4fb4-b461-3f6af4464599",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(script_directory, file)\n",
    "        read_text_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0431fbef-6069-4364-a84e-cea9a0a035e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcc300a8-fef0-460c-9964-9f1edaadccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(tokens):\n",
    "    characters_to_remove = [',', '.', ' ']\n",
    "    for token in tokens:\n",
    "        if token in characters_to_remove:\n",
    "            tokens.remove(token)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34a43852-fe9e-4ab1-9193-d36e033172ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "469533dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Darshan\n",
      "[nltk_data]     Mahajan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ca840",
   "metadata": {},
   "source": [
    "Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9ad99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_list = []\n",
    "for word in all_tokens:\n",
    "    if word.casefold() not in stop_words:\n",
    "         filtered_list.append(word)\n",
    "\n",
    "all_tokens = filtered_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "018561a8-7db1-49c9-baa6-0f0e86ca8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Darshan Mahajan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b016296",
   "metadata": {},
   "source": [
    "Part of speech is a grammatical term that deals with the roles words play when you use them together in sentences. \n",
    "Tagging parts of speech, or POS tagging, is the task of labeling the words in your text according to their part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e948919-e39e-46aa-9203-4aa52cbf0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nltk.pos_tag(all_tokens)\n",
    "# pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7663144",
   "metadata": {},
   "source": [
    "Tags    Deal with\n",
    "JJ      Adjectives\n",
    "NN      Nouns\n",
    "RB\t    Adverbs\n",
    "PRP\t    Pronouns\n",
    "VB\t    Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82b48b67-3cf8-4c46-ae3c-dda1d79bbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac981be",
   "metadata": {},
   "source": [
    "Stemming is a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.” Stemming allows you to zero in on the basic meaning of a word rather than all the details of how it’s being used. NLTK has more than one stemmer, but you’ll be using the Porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ede0efa-0228-431e-81b6-e7b3dcd89c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for w in all_tokens:\n",
    "    dict[w] = ps.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "453746c7-7d64-4a4c-b2c5-54b3d6d16266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3c1cd",
   "metadata": {},
   "source": [
    "Understemming and overstemming are two ways stemming can go wrong:\n",
    "\n",
    "1. Understemming happens when two related words should be reduced to the same stem but aren’t. This is a false negative.\n",
    "\n",
    "2. Overstemming happens when two unrelated words are reduced to the same stem even though they shouldn’t be. This is a false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3796d6c-7d5f-40bb-b962-8bdc0ef9b6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Darshan\n",
      "[nltk_data]     Mahajan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Darshan\n",
      "[nltk_data]     Mahajan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4975b0",
   "metadata": {},
   "source": [
    " Like stemming, lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like 'discoveri'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "830ce6a0-7bcb-400c-b296-4587fabb3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c70e04d6-f617-43cf-a246-ce582025e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_dict = {}\n",
    "for w in all_tokens:\n",
    "    lem_dict[w] = lm.lemmatize(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "56530d1f-6301-4e71-83d8-0787e42d2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lem_dict)\n",
    "# print(len(lem_dict))\n",
    "# print(len(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93307e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTF(token):\n",
    "    term_freq = {}\n",
    "    for word in token:\n",
    "        if word not in term_freq:\n",
    "            term_freq[word] = token.count(word) / len(token)\n",
    "\n",
    "    return term_freq\n",
    "\n",
    "calculateTF(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854669e0",
   "metadata": {},
   "source": [
    "TF = (Number of times the term appears in the document) / (Total number of terms in the document)\n",
    "\n",
    "IDF = log((Total number of docs) / (Number of docs that contain the term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55baa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_doc_frequency = {}\n",
    "\n",
    "for w in all_tokens:\n",
    "    tot_docs_having_w = 0\n",
    "    \n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(script_directory, file)\n",
    "            with open(file_path, 'r') as f: \n",
    "                word = f.read()\n",
    "                if w in nltk.word_tokenize(word):\n",
    "                    tot_docs_having_w += 1\n",
    "    \n",
    "    inverse_doc_frequency[w] =  np.log10(10 / tot_docs_having_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "104a1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inverse_doc_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06f67b",
   "metadata": {},
   "source": [
    "1. IDF value of 0: This typically occurs when a term (word) is present in all documents within the corpus. In such cases, the IDF value is set to 0 to prevent division by zero when calculating the TF-IDF score. A term with an IDF value of 0 indicates that it is not discriminative or distinctive across documents and therefore may not contribute much to distinguishing documents.\n",
    "2. IDF value of 1: This happens when the term appears in only one document in the corpus. In this case, the IDF value is often adjusted to 1 to indicate that the term is relatively rare but still present in the corpus. However, it doesn't provide significant discriminative power compared to terms with higher IDF values.\n",
    "3. Other IDF values: Terms with IDF values other than 0 or 1 indicate their rarity or uniqueness within the corpus. Higher IDF values indicate that the term is rare across documents and therefore potentially more discriminative. Terms with higher IDF values contribute more to the TF-IDF score and are considered more important in distinguishing documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609faa14",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical measure that indicates the importance of a word in a document taking into account how frequent the word is in other documents in the same corpus. It consists of multiplying the term frequency (TF) by the inverse document frequency (IDF), which is the logarithm of the total number of documents divided by the number of documents containing the term. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
